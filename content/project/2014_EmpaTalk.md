+++
# Date this page was created.
date = "2014-01-01"

# Project title.
title = "Empa Talk"

# Project summary to display on homepage.
summary = "CHI EA'14"

# Optional image to display on homepage (relative to `static/img/` folder).
image_preview = "thumbnail/EmpaTalk.jpg"

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ["research", "HCI", "Physiological Signal"]

# Optional external URL for project (replaces project detail page).
external_link = ""

# Does the project detail page use math formatting?
math = false

# Optional featured image (relative to `static/img/` folder).
[header]
#image = "headers/bubbles-wide.jpg"
#caption = "My caption :smile:"

+++
### Abstract
We present a novel approach that allows the user to feel the other’s emotional status while communicating with each other in a video chat. The video chat is composed of physiological sensors and multimodal displays. In our first prototype, we employed a Galvanic Skin Response (GSR) sensor and a Blood Volume Pulse (BVP) sensor as they were crucial indications to emotion. A vibrotactile motor and a RGB Led were also used in order to convey the other’s emotion on one’s wrist. Along with the hardware part, we implemented intuitive software for processing, transmitting, and displaying bio feedback data.

[Poster](/files/pdf/chi2014wip_poster.pdf)   
[Extended Abstract](/files/pdf/chi2014wip_empatalk.pdf)
